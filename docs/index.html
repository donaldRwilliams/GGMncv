<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>GGMncv • GGMncv</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="bootstrap-toc.css">
<script src="bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="pkgdown.css" rel="stylesheet">
<script src="pkgdown.js"></script><meta property="og:title" content="GGMncv">
<meta property="og:description" content="Estimate Gaussian graphical models with noncovex penalties">
<meta property="og:image" content="https://donaldrwilliams.github.io/GGMncv/reference/figures/hex.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@wdonald_1985">
<meta name="twitter:site" content="@wdonald_1985">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-home">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="index.html">GGMncv</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">2.0.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="articles/cpu_time.html">NCT: CPU Time</a>
    </li>
    <li>
      <a href="articles/high_dim.html">High Dimensional Data: Must Read!!</a>
    </li>
    <li>
      <a href="articles/nct_custom.html">Custom Network Comparison Tests</a>
    </li>
    <li>
      <a href="articles/null_dist.html">NCT: Null Distributions</a>
    </li>
    <li>
      <a href="articles/sign_restrict.html">Positive Manifold (Sign Restriction)</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/donaldRwilliams/GGMncv/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="contents col-md-9">

<p><img src="reference/figures/hex.png" width="200"></p>
<div id="ggmncv-gaussian-graphical-models-with-non-convex-penalties" class="section level1">
<div class="page-header"><h1 class="hasAnchor">
<a href="#ggmncv-gaussian-graphical-models-with-non-convex-penalties" class="anchor"></a>GGMncv: Gaussian Graphical Models with Non-Convex Penalties</h1></div>
<p><a href="https://cran.r-project.org/package=GGMncv"><img src="http://www.r-pkg.org/badges/version/GGMncv" alt="CRAN Version"></a> <a href="https://cran.r-project.org/package=GGMncv"><img src="https://cranlogs.r-pkg.org/badges/GGMncv" alt="Downloads"></a> <a href="https://travis-ci.org/donaldRwilliams/GGMncv"><img src="https://travis-ci.org/donaldRwilliams/GGMncv.svg?branch=master" alt="Build Status"></a></p>
<p>The primary goal of GGMncv is to provide non-convex penalties for estimating Gaussian graphical models. These are known to overcome the various limitations of lasso (least absolute shrinkage “screening” operator), including inconsistent model selection (Zhao and Yu 2006), biased estimates (C.-H. Zhang 2010)<span id="a1"><a href="#f1"><span class="math display">1</span></a></span>, and a high false positive rate (see for example Williams and Rast 2020; Williams et al. 2019).</p>
<p>Note that these limitations of lasso are well-known. In the case of false positives, for example, it has been noted that</p>
<blockquote>
<p>The lasso is doing variable screening and, hence, I suggest that we interpret the second ‘s’ in lasso as ‘screening’ rather than ‘selection.’ Once we have the screening property, the task is to remove the false positive selections (p. 278, Tibshirani 2011).</p>
</blockquote>
<p>An additional goal of <strong>GGMncv</strong> is to provide methods for making statistical inference in <strong>regularized</strong> Gaussian graphical models. This is accomplished with the de-sparsified graphical lasso estimator introduced in Jankova and Van De Geer (2015). This is described in the section <a href="#de-sparsified-estimator">De-Sparsified Estimator</a>. The next <a href="#comparing-ggms">section</a> shows how the de-sparsified estimator can be used to compare GGMs.</p>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h2>
<p>You can install the released version (<code>2.0.0</code>) of <strong>GGMncv</strong> from <a href="https://CRAN.R-project.org">CRAN</a> with:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"GGMncv"</span><span class="op">)</span></code></pre></div>
<p>You can install development version from <a href="https://github.com/">GitHub</a> with:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># install.packages("devtools")</span>
<span class="fu">devtools</span><span class="fu">::</span><span class="fu"><a href="https://devtools.r-lib.org//reference/remote-reexports.html">install_github</a></span><span class="op">(</span><span class="st">"donaldRwilliams/GGMncv"</span><span class="op">)</span></code></pre></div>
</div>
<div id="penalties" class="section level2">
<h2 class="hasAnchor">
<a href="#penalties" class="anchor"></a>Penalties</h2>
<p>The following are implemented in <code>GGMncv</code>:</p>
<ol>
<li><p>Atan (<code>penalty = "atan"</code>; Wang and Zhu (2016)). This is currently the default.</p></li>
<li><p>Seamless <em>L</em><sub>0</sub> (<code>penalty = "selo"</code>; Dicker, Huang, and Lin (2013))</p></li>
<li><p>Exponential (<code>penalty = "exp"</code>; Wang, Fan, and Zhu (2018))</p></li>
<li><p>Smooth integration of counting and absolute deviation (<code>penalty = "sica"</code>; Lv and Fan (2009))</p></li>
<li><p>Log (<code>penalty = "log"</code>; Mazumder, Friedman, and Hastie (2011))</p></li>
<li><p><em>L</em><sub>q</sub> (<code>penalty = "lq"</code>, <em>0</em> &lt; <em>q</em> &lt; <em>1</em>; e.g., Knight and Fu (2000))</p></li>
<li><p>Smoothly clipped absolute deviation (<code>penalty = "scad"</code>; Fan and Li (2001))</p></li>
<li><p>Minimax concave penalty (<code>penalty = "mcp"</code>; C.-H. Zhang (2010))</p></li>
<li><p>Adaptive lasso (<code>penalty = "adapt"</code>; Zou (2006))</p></li>
<li><p>Lasso (<code>penalty = "lasso"</code>; Tibshirani (1996))</p></li>
</ol>
<p>Options 1-5 are continuous approximations to the <em>L</em><sub>0</sub> penalty, that is, best subsets model selection. However, the solution is computationally efficient and solved with the local linear approximation described in Fan, Feng, and Wu (2009) or the one-step approach described in Zou and Li (2008).</p>
<div id="penalty-function" class="section level3">
<h3 class="hasAnchor">
<a href="#penalty-function" class="anchor"></a>Penalty Function</h3>
<p>The basic idea of these penalties is to provide “tapering,” in which regularization is less severe for large effects. The following is an example for the Atan penalty (<embed src="https://latex.codecogs.com/svg.latex?%5Cgamma"></embed> is the hyperparameter)</p>
<p><img src="reference/figures/pen_func.png"></p>
<p>Note that (1) the penalty provides a “smooth” function that ranges from <em>L</em><sub>0</sub> (best subsets) and <em>L</em><sub>1</sub> (lasso) regularization; and (2) the penalty “tapers” off for large effects.</p>
</div>
<div id="computation" class="section level3">
<h3 class="hasAnchor">
<a href="#computation" class="anchor"></a>Computation</h3>
<p>Computing the non-convex solution is a challenging task. However, section 3.3 in Zou and Li (2008) indicates that the one-step approach is a viable <strong>approximation</strong> for a variety of non-convex penalties, assuming the initial estimates are “good enough”<span id="a2"><a href="#f2"><span class="math display">2</span></a></span>. To this end, the initial values can either be the sample based inverse covariance matrix or a custom matrix specified with <code>initial</code>.</p>
</div>
</div>
<div id="tuning-parameter" class="section level2">
<h2 class="hasAnchor">
<a href="#tuning-parameter" class="anchor"></a>Tuning Parameter</h2>
<div id="selection" class="section level3">
<h3 class="hasAnchor">
<a href="#selection" class="anchor"></a>Selection</h3>
<p>The tuning parameter can be selected with several information criteria (IC), including <code>aic</code>, <code>bic</code> (currently the default),<code>ebic</code>, <code>ric</code>, in addition to any of the <em>generalized</em> information criteria provided in section 5 of Kim, Kwon, and Choi (2012).</p>
<p>Information criterion can be understood as penalizing the likelihood, with the difference being in the severity of the penalty. -2 times the log-likelihood is defined as</p>
<p><embed src="https://latex.codecogs.com/svg.latex?-2l_n%28%5Chat%7B%5Cboldsymbol%7B%5CTheta%7D%7D%29%20%3D%20-2%20%5CBig%5B%5Cfrac%7Bn%7D%7B2%7D%20%5Ctext%7Blogdet%7D%5Chat%7B%5Cboldsymbol%7B%5CTheta%7D%7D%20-%20%5Ctext%7Btr%7D%28%5Chat%7B%5Ctext%7B%5Cbf%7BS%7D%7D%7D%20%5Chat%7B%5Cboldsymbol%7B%5CTheta%7D%7D%29%20%5CBig%5D"></embed></p>
<p>where <embed src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cboldsymbol%7B%5CTheta%7D%7D"></embed> is the estimated precision matrix and <embed src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Ctext%7B%5Cbf%7BS%7D%7D%7D"></embed> is the sample based covariance matrix. The included criterion then add the following penalties:</p>
<ul>
<li>GIC<sub>1</sub> (BIC): <embed src="https://latex.codecogs.com/svg.latex?%7C%5Ctext%7B%5Cbf%7BE%7D%7D%7C%20%5Ctext%7Blog%7D%28n%29"></embed>
</li>
</ul>
<p>Note that <embed src="https://latex.codecogs.com/svg.latex?%7C%5Ctext%7B%5Cbf%7BE%7D%7D%7C"></embed> refers to the cardinality of the edge set, that is, the number of edges.</p>
<ul>
<li>GIC<sub>2</sub>: <embed src="https://latex.codecogs.com/svg.latex?%7C%5Ctext%7B%5Cbf%7BE%7D%7D%7C%20p%5E%7B1/3%7D"></embed>
</li>
</ul>
<p><em>p</em> denotes the number of nodes or columns in the data matrix.</p>
<ul>
<li><p>GIC<sub>3</sub> (RIC): <embed src="https://latex.codecogs.com/svg.latex?%7C%5Ctext%7B%5Cbf%7BE%7D%7D%7C%202%20%5Ctext%7Blog%7D%28p%29"></embed></p></li>
<li><p>GIC<sub>4</sub>: <embed src="https://latex.codecogs.com/svg.latex?2%7C%5Ctext%7B%5Cbf%7BE%7D%7D%7C%5Ctext%7Blog%7D%28p%29%20+%20%5Ctext%7Bloglog%7D%28p%29"></embed></p></li>
<li><p>GIC<sub>5</sub> (BIC with divergent dimensions): <embed src="https://latex.codecogs.com/svg.latex?%7C%5Ctext%7B%5Cbf%7BE%7D%7D%7C%20%5Ctext%7Bloglog%7D%28n%29%5Ctext%7Blog%7D%28p%29"></embed></p></li>
<li><p>GIC<sub>6</sub>: <embed src="https://latex.codecogs.com/svg.latex?%7C%5Ctext%7B%5Cbf%7BE%7D%7D%7C%20%5Ctext%7Blog%7D%28n%29%5Ctext%7Blog%7D%28p%29"></embed></p></li>
<li><p>AIC: <embed src="https://latex.codecogs.com/svg.latex?2%7C%5Ctext%7B%5Cbf%7BE%7D%7D%7C"></embed></p></li>
</ul>
<p>Although cross-validation is not implemented for selecting the tuning parameter, AIC can be used to approximate leave-one-out cross-validation.</p>
<ul>
<li>EBIC: <embed src="https://latex.codecogs.com/svg.latex?%7C%5Ctext%7B%5Cbf%7BE%7D%7D%7C%5Ctext%7Blog%7D%28n%29%20+%204%20%7C%5Ctext%7B%5Cbf%7BE%7D%7D%7C%20%5Cgamma%20%5Ctext%7Blog%7D%28p%29%2C%5C%3B%20%5C0%20%5Cleq%20%5Cgamma%20%5Cleq%201"></embed>
</li>
</ul>
<p>The tuning parameter is selected by setting <code>select = TRUE</code> and then the desired IC with, for example, <code>ic = "gic_3"</code>.</p>
</div>
<div id="tuning-free" class="section level3">
<h3 class="hasAnchor">
<a href="#tuning-free" class="anchor"></a>Tuning Free</h3>
<p>A tuning free option is also available. This is accomplished by setting the tuning parameter to <embed src="https://latex.codecogs.com/svg.latex?%5Csmall%20%5Csqrt%7Blog%28p%29/n%7D"></embed> (see for example R. Zhang, Ren, and Chen 2018; Li et al. 2015; Jankova and Van De Geer 2015) and then selecting <embed src="https://latex.codecogs.com/svg.latex?%5Cgamma"></embed></p>
</div>
</div>
<div id="example-structure-learning" class="section level2">
<h2 class="hasAnchor">
<a href="#example-structure-learning" class="anchor"></a>Example: Structure Learning</h2>
<p>A GGM can be fitted as follows</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">GGMncv</span><span class="op">)</span>

<span class="co"># data</span>
<span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu">GGMncv</span><span class="fu">::</span><span class="va"><a href="reference/ptsd.html">ptsd</a></span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span>

<span class="co"># polychoric</span>
<span class="va">R</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">Y</span>, method <span class="op">=</span> <span class="st">"spearman"</span><span class="op">)</span>

<span class="co"># fit model</span>
<span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/ggmncv.html">ggmncv</a></span><span class="op">(</span>R <span class="op">=</span> <span class="va">R</span>, n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span>, 
              penalty <span class="op">=</span> <span class="st">"atan"</span><span class="op">)</span>

<span class="co"># print</span>
<span class="va">fit</span>

<span class="co">#&gt;       1     2     3     4     5     6     7     8     9    10</span>
<span class="co">#&gt; 1  0.000 0.255 0.000 0.309 0.101 0.000 0.000 0.000 0.073 0.000</span>
<span class="co">#&gt; 2  0.255 0.000 0.485 0.000 0.000 0.000 0.122 0.000 0.000 0.000</span>
<span class="co">#&gt; 3  0.000 0.485 0.000 0.185 0.232 0.000 0.000 0.000 0.000 0.000</span>
<span class="co">#&gt; 4  0.309 0.000 0.185 0.000 0.300 0.000 0.097 0.000 0.000 0.243</span>
<span class="co">#&gt; 5  0.101 0.000 0.232 0.300 0.000 0.211 0.166 0.000 0.000 0.000</span>
<span class="co">#&gt; 6  0.000 0.000 0.000 0.000 0.211 0.000 0.234 0.079 0.000 0.000</span>
<span class="co">#&gt; 7  0.000 0.122 0.000 0.097 0.166 0.234 0.000 0.000 0.000 0.000</span>
<span class="co">#&gt; 8  0.000 0.000 0.000 0.000 0.000 0.079 0.000 0.000 0.000 0.114</span>
<span class="co">#&gt; 9  0.073 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.261</span>
<span class="co">#&gt; 10 0.000 0.000 0.000 0.243 0.000 0.000 0.000 0.114 0.261 0.000</span></code></pre></div>
<p>Note that the object <code>fit</code> can be plotted with the <code>R</code> package <a href="https://CRAN.R-project.org/package=qgraph"><strong>qgraph</strong></a>.</p>
</div>
<div id="example-out-of-sample-prediction" class="section level2">
<h2 class="hasAnchor">
<a href="#example-out-of-sample-prediction" class="anchor"></a>Example: Out-of-Sample Prediction</h2>
<p>The <strong>GGMncv</strong> package can also be used for prediction, given the correspondence between the inverse covariance matrix and multiple regression (Kwan 2014).</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">Sachs</span><span class="op">)</span>

<span class="co"># test data</span>
<span class="va">Ytest</span> <span class="op">&lt;-</span> <span class="va">Y</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">100</span>,<span class="op">]</span>

<span class="co"># training data</span>
<span class="va">Ytrain</span> <span class="op">&lt;-</span> <span class="va">Y</span><span class="op">[</span><span class="fl">101</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span>,<span class="op">]</span>

<span class="co"># default: atan and tuning free</span>
<span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/ggmncv.html">ggmncv</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">Ytrain</span><span class="op">)</span>, 
              n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Ytrain</span><span class="op">)</span><span class="op">)</span>

<span class="co"># predict</span>
<span class="va">pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">fit</span>, train_data <span class="op">=</span> <span class="va">Ytrain</span>, newdata <span class="op">=</span> <span class="va">Ytest</span><span class="op">)</span>

<span class="co"># print mse</span>
<span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="op">(</span><span class="va">pred</span> <span class="op">-</span> <span class="va">Ytest</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span>, <span class="fl">2</span>, <span class="va">mean</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span>

<span class="co">#&gt;  Raf  Erk Plcg  PKC  PKA PIP2 PIP3  Mek  P38  Jnk  Akt </span>
<span class="co">#&gt; 0.18 0.27 0.59 0.42 0.39 0.47 0.69 0.16 0.15 0.69 0.26 </span></code></pre></div>
</div>
<div id="solution-path" class="section level2">
<h2 class="hasAnchor">
<a href="#solution-path" class="anchor"></a>Solution Path</h2>
<p>When <code>select = 'lambda'</code>, the solution path for the partial correlations can be plotted.</p>
<div id="atan-penalty" class="section level3">
<h3 class="hasAnchor">
<a href="#atan-penalty" class="anchor"></a>Atan Penalty</h3>
<p>Here is the current default penalty</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># data</span>
<span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">ptsd</span>

<span class="co"># fit model</span>
<span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">GGMncv</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span>, n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span>, 
              select <span class="op">=</span> <span class="cn">TRUE</span>, 
              store <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
              
<span class="co"># plot path</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">fit</span>, 
     alpha <span class="op">=</span> <span class="fl">0.75</span><span class="op">)</span></code></pre></div>
<p><img src="reference/figures/atan_path.png"></p>
<p>The dotted line is denotes the selected lambda. Notice how the larger partial correlations “escape” regularization, at least to some degree, compared to the smaller partial correlations.</p>
</div>
<div id="lasso-penalty" class="section level3">
<h3 class="hasAnchor">
<a href="#lasso-penalty" class="anchor"></a>Lasso Penalty</h3>
<p>Next <em>L</em><sub>1</sub> regularization is implemented by setting <code>penalty = "lasso"</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> ptsd</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">GGMncv</span>(<span class="fu">cor</span>(Y), <span class="at">n =</span> <span class="fu">nrow</span>(Y), </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">penalty =</span> <span class="st">"lasso"</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>              <span class="at">store =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># plot path</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">alpha =</span> <span class="fl">0.75</span>)</span></code></pre></div>
<p><img src="reference/figures/lasso_path.png"></p>
<p>This solution is much different than above. For example, it is clear that the large partial correlations are heavily penalized, whereas this was not so for the atan penalty. The reason this is not ideal is that, if the partial correlations are large, it makes sense that they should not be penalized that much. This property of non-convex regularization should provide <em>nearly</em> unbiased estimates, which can improve, say, predictive accuracy.</p>
<p>Also notice that the atan penalty provides a sparser solution.</p>
</div>
</div>
<div id="bootstrapping" class="section level2">
<h2 class="hasAnchor">
<a href="#bootstrapping" class="anchor"></a>Bootstrapping</h2>
<p><strong>GGMncv</strong> does not provide confidence intervals based on bootstrapping.<br>
This is because, in general, “confidence” intervals from penalized approaches do not have the correct properties to be considered confidence intervals <a href="https://en.wikipedia.org/wiki/Confidence_interval">(see Wikipedia)</a>. This sentiment is echoed in Section 3.1, “Why standard bootstrapping and subsampling do not work,” of Bühlmann, Kalisch, and Meier (2014):</p>
<blockquote>
<p>The (limiting) distribution of such a sparse estimator is non-Gaussian with point mass at zero, and this is the reason why standard bootstrap or subsampling techniques do not provide valid confidence regions or p-values (pp. 7-8).</p>
</blockquote>
<p>For this reason, it is common to <strong>not</strong> provide standard errors (and thus confidence intervals) for penalized models <span id="a3"><a href="#f3"><span class="math display">3</span></a></span>. For example, this is from the <strong>penalized</strong> <code>R</code> package:</p>
<blockquote>
<p>It is a very natural question to ask for standard errors of regression coefficients or other estimated quantities. In principle such standard errors can easily be calculated, e.g. using the bootstrap. Still, this package deliberately does not provide them. The reason for this is that standard errors are not very meaningful for strongly biased estimates such as arise from penalized estimation methods (p.18, Goeman, Meijer, and Chaturvedi 2018)</p>
</blockquote>
<p>However, <strong>GGMncv</strong> does include the so-called variable inclusion “probability” for each relation (see p. 1523 in Bunea et al. 2011; and Figure 6.7 in Hastie, Tibshirani, and Wainwright 2015). These are computed using a non-parametric bootstrap strategy.</p>
<p>Additionally, more recent work does allow for obtaining confidence intervals and <em>p</em>-values with the de-sparsified method. For the graphical lasso, the former are not available for the partial correlations so currently only <em>p</em>-values are provided <a href="#statistical-inference">(Statistical Inferece)</a>.</p>
<div id="variable-inclusion-probability" class="section level3">
<h3 class="hasAnchor">
<a href="#variable-inclusion-probability" class="anchor"></a>Variable Inclusion “Probability”</h3>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># data</span>
<span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu">GGMncv</span><span class="fu">::</span><span class="va"><a href="reference/ptsd.html">ptsd</a></span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span>

<span class="co"># edge inclusion</span>
<span class="va">eips</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/boot_eip.html">boot_eip</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span>

<span class="co"># plot</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">eips</span>, size <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></code></pre></div>
<p><img src="reference/figures/vip.png"></p>
</div>
</div>
<div id="statistical-inference" class="section level2">
<h2 class="hasAnchor">
<a href="#statistical-inference" class="anchor"></a>Statistical Inference</h2>
<p>It might be tempting to think these approaches lead to rich <em>inference</em>. This would be a mistake–they suffer from all of the problems inherent to automated procedures for model selection (e.g., Berk et al. 2013; Lee et al. 2016).</p>
<p>And note that:</p>
<ol>
<li><p>Simply <strong>not</strong> detecting an effect does not provide evidence for the null hypothesis.</p></li>
<li><p>There is <strong>not</strong> necessarily a difference between an effect that was and an effect that was not detected.</p></li>
</ol>
<p>Supporting these claims would require a valid confidence interval that has been corrected for model selection and/or regularization. With these caveats in mind, data driven model selection in <strong>GGMncv</strong> can be used for explicit data mining or prediction.</p>
<div id="de-sparsified-estimator" class="section level3">
<h3 class="hasAnchor">
<a href="#de-sparsified-estimator" class="anchor"></a>De-Sparsified Estimator</h3>
<p>To make inference, <strong>GGMncv</strong> computes the de-sparsified estimator, <embed src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Ctext%7B%5Cbf%7BT%7D%7D%7D"></embed>, introduced in Jankova and Van De Geer (2015), that is</p>
<p><embed src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Ctext%7B%5Cbf%7BT%7D%7D%7D%20%3D%202%5Chat%7B%5Cboldsymbol%7B%5CTheta%7D%7D%20-%20%5Chat%7B%5Cboldsymbol%7B%5CTheta%7D%7D%20%5Chat%7B%5Ctext%7B%5Cbf%7BR%7D%7D%7D%5Chat%7B%5Cboldsymbol%7B%5CTheta%7D%7D"></embed></p>
<p>where <embed src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cboldsymbol%7B%5CTheta%7D%7D"></embed> is the estimated precision matrix and <embed src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Ctext%7B%5Cbf%7BR%7D%7D%7D"></embed> is the sample based correlation matrix. As the name implies, this removes the zeros and corrects the bias from regularization. The asymptotic variance is then given as</p>
<p><embed src="https://latex.codecogs.com/svg.latex?%5Ctext%7BVar%7D%5B%5Chat%7B%5Ctext%7B%5Cbf%7BT%7D%7D%7D%5D%20%3D%20%7B%5Ctext%7Bdiag%7D%28%5Chat%7B%5Ctext%7B%5Cbf%7BT%7D%7D%7D%29%20%5Ctext%7Bdiag%7D%28%5Chat%7B%5Ctext%7B%5Cbf%7BT%7D%7D%7D%29%5E%5Cprime%20+%20%5Chat%7B%5Ctext%7B%5Cbf%7BT%7D%7D%7D%5E2%7D"></embed></p>
<p>which readily allows for computing <em>p</em>-values for each off-diagonal element of the de-sparsified estimator.</p>
<p>This is implemented with</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># data</span>
<span class="va">Y</span> <span class="op">&lt;-</span> <span class="va">ptsd</span>

<span class="co"># fit model</span>
<span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/ggmncv.html">ggmncv</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span>, n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">)</span>

<span class="co"># make inference</span>
<span class="va">fdr_ggm</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/inference.html">inference</a></span><span class="op">(</span><span class="va">fit</span>, method <span class="op">=</span> <span class="st">"fdr"</span><span class="op">)</span>

<span class="co"># print</span>
<span class="va">fdr_ggm</span>

<span class="co">#&gt; Statistical Inference</span>
<span class="co">#&gt; fdr: 0.05</span>
<span class="co">#&gt; ---</span>

<span class="co">#&gt;   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</span>
<span class="co">#&gt; 1  0 1 0 1 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0</span>
<span class="co">#&gt; 2  1 0 1 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0</span>
<span class="co">#&gt; 3  0 1 0 1 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0</span>
<span class="co">#&gt; 4  1 0 1 0 1 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0</span>
<span class="co">#&gt; 5  0 0 0 1 0 1 0 0 0  0  0  0  0  0  1  0  0  1  0  0</span>
<span class="co">#&gt; 6  0 0 0 0 1 0 1 0 0  0  0  0  0  0  0  0  0  0  0  0</span>
<span class="co">#&gt; 7  0 0 0 0 0 1 0 0 0  0  0  1  0  0  0  0  0  0  0  0</span>
<span class="co">#&gt; 8  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0</span>
<span class="co">#&gt; 9  0 0 0 0 0 0 0 0 0  0  1  1  0  0  0  0  0  0  0  0</span>
<span class="co">#&gt; 10 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0</span>
<span class="co">#&gt; 11 1 0 0 0 0 0 0 0 1  1  0  0  0  0  1  0  0  0  0  0</span>
<span class="co">#&gt; 12 0 0 0 0 0 0 1 0 1  0  0  0  1  0  0  0  0  0  1  0</span>
<span class="co">#&gt; 13 0 0 0 0 0 0 0 0 0  0  0  1  0  1  0  0  0  0  1  0</span>
<span class="co">#&gt; 14 0 0 0 0 0 0 0 0 0  0  0  0  1  0  0  0  0  0  0  0</span>
<span class="co">#&gt; 15 0 0 0 0 1 0 0 0 0  0  1  0  0  0  0  1  0  0  0  0</span>
<span class="co">#&gt; 16 0 1 0 0 0 0 0 0 0  0  0  0  0  0  1  0  0  0  0  0</span>
<span class="co">#&gt; 17 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  1  0  0</span>
<span class="co">#&gt; 18 0 0 0 0 1 0 0 0 0  0  0  0  0  0  0  0  1  0  0  0</span>
<span class="co">#&gt; 19 0 0 0 0 0 0 0 0 0  0  0  1  1  0  0  0  0  0  0  1</span>
<span class="co">#&gt; 20 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  1  0</span></code></pre></div>
<p>Note that the object <code>fdr_ggm</code> includes the de-sparsified precision matrix, the partial correlation matrix, and <em>p</em>-values for each relation. Furthermore, there is a function called <code><a href="reference/desparsify.html">desparsify()</a></code> that can be used to obtain the de-sparsified estimator without computing the <em>p</em>-values.</p>
</div>
</div>
<div id="comparing-ggms" class="section level2">
<h2 class="hasAnchor">
<a href="#comparing-ggms" class="anchor"></a>Comparing GGMs</h2>
<p>Because the de-sparsified estimator provides the variance for each relation, this readily allows for comparing GGMs. This is accomplished by computing the difference and then the variance of that difference. Assuming there is two groups, <code>Y_g1</code> and <code>Y_g2</code>, this is implemented with</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fit1</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/ggmncv.html">ggmncv</a></span><span class="op">(</span><span class="va">Y_g1</span>, n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Y_g1</span><span class="op">)</span><span class="op">)</span>
<span class="va">fit2</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/ggmncv.html">ggmncv</a></span><span class="op">(</span><span class="va">Y_g2</span>, n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Y_g2</span><span class="op">)</span><span class="op">)</span>

<span class="va">ggm_diff</span> <span class="op">&lt;-</span> <span class="fu">ggm_compare</span><span class="op">(</span><span class="va">fit1</span>, <span class="va">fit2</span><span class="op">)</span></code></pre></div>
<p>The object <code>ggm_diff</code> includes the partial correlation differences, <em>p</em>-values, and the adjacency matrix.</p>
</div>
<div id="citing-ggmncv" class="section level2">
<h2 class="hasAnchor">
<a href="#citing-ggmncv" class="anchor"></a>Citing <strong>GGMncv</strong>
</h2>
<p>It is important to note that <strong>GGMncv</strong> merely provides a software implementation of other researchers work. There are no methological innovations, although this is the most comprehensive <code>R</code> package for estimating GGMs with non-convex penalties. Hence, in addition to citing the package <code>citation("GGMncv")</code>, it is important to give credit to the primary sources. The references can be found in <a href="#penalties">(Penalties)</a>.</p>
<p>Additionally, please cite Williams (2020) which is survey of these approaches that is meant to accompany <strong>GGMncv</strong>.</p>
</div>
<div id="footnotes" class="section level2">
<h2 class="hasAnchor">
<a href="#footnotes" class="anchor"></a>Footnotes</h2>
<ol>
<li><p><span id="f1"></span> Note that the penalties in <strong>GGMncv</strong> should provide <em>nearly</em> unbiased estimates <a href="#a1">(return)</a>.</p></li>
<li><p><span id="f2"></span> In low-dimensional settings, assuming that <em>n</em> is sufficiently larger than <em>p</em>, the sample covariance matrix provides adequate initial estimates. In high-dimensional settings (<em>n</em> &lt; <em>p</em>), the initial estimates are obtained from lasso <a href="#a2">(return)</a>.</p></li>
<li><p><span id="f3"></span> It is possible to compute confidence intervals for lasso with the methods included in the <strong>SILGGM</strong> <code>R</code> package (R. Zhang, Ren, and Chen 2018). These do not use the bootstrap <a href="#a3">(return)</a>.</p></li>
</ol>
</div>
<div id="references" class="section level2">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-berk2013valid" class="csl-entry">
Berk, Richard, Lawrence Brown, Andreas Buja, Kai Zhang, Linda Zhao, et al. 2013. “Valid Post-Selection Inference.” <em>The Annals of Statistics</em> 41 (2): 802–37.
</div>
<div id="ref-Buhlmann2014" class="csl-entry">
Bühlmann, Peter, Markus Kalisch, and Lukas Meier. 2014. “<span class="nocase">High-Dimensional Statistics with a View Toward Applications in Biology</span>.” <em>Annual Review of Statistics and Its Application</em> 1 (1): 255–78. <a href="https://doi.org/10.1146/annurev-statistics-022513-115545" class="uri">https://doi.org/10.1146/annurev-statistics-022513-115545</a>.
</div>
<div id="ref-bunea2011penalized" class="csl-entry">
Bunea, Florentina, Yiyuan She, Hernando Ombao, Assawin Gongvatana, Kate Devlin, and Ronald Cohen. 2011. “Penalized Least Squares Regression Methods and Applications to Neuroimaging.” <em>NeuroImage</em> 55 (4): 1519–27.
</div>
<div id="ref-dicker2013variable" class="csl-entry">
Dicker, Lee, Baosheng Huang, and Xihong Lin. 2013. “Variable Selection and Estimation with the Seamless-l 0 Penalty.” <em>Statistica Sinica</em>, 929–62.
</div>
<div id="ref-fan2009network" class="csl-entry">
Fan, Jianqing, Yang Feng, and Yichao Wu. 2009. “Network Exploration via the Adaptive LASSO and SCAD Penalties.” <em>The Annals of Applied Statistics</em> 3 (2): 521.
</div>
<div id="ref-fan2001variable" class="csl-entry">
Fan, Jianqing, and Runze Li. 2001. “Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties.” <em>Journal of the American Statistical Association</em> 96 (456): 1348–60.
</div>
<div id="ref-goeman2018l1" class="csl-entry">
Goeman, Jelle, Rosa Meijer, and Nimisha Chaturvedi. 2018. “L1 and L2 Penalized Regression Models.” <em>Vignette R Package Penalized.</em>
</div>
<div id="ref-hastie2015statistical" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Martin Wainwright. 2015. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. CRC press.
</div>
<div id="ref-jankova2015confidence" class="csl-entry">
Jankova, Jana, and Sara Van De Geer. 2015. “Confidence Intervals for High-Dimensional Inverse Covariance Estimation.” <em>Electronic Journal of Statistics</em> 9 (1): 1205–29.
</div>
<div id="ref-kim2012consistent" class="csl-entry">
Kim, Yongdai, Sunghoon Kwon, and Hosik Choi. 2012. “Consistent Model Selection Criteria on High Dimensions.” <em>The Journal of Machine Learning Research</em> 13: 1037–57.
</div>
<div id="ref-knight2000asymptotics" class="csl-entry">
Knight, Keith, and Wenjiang Fu. 2000. “Asymptotics for Lasso-Type Estimators.” <em>Annals of Statistics</em>, 1356–78.
</div>
<div id="ref-kwan2014regression" class="csl-entry">
Kwan, Clarence CY. 2014. “A Regression-Based Interpretation of the Inverse of the Sample Covariance Matrix.” <em>Spreadsheets in Education</em> 7 (1): 4613.
</div>
<div id="ref-lee2016exact" class="csl-entry">
Lee, Jason D, Dennis L Sun, Yuekai Sun, Jonathan E Taylor, et al. 2016. “Exact Post-Selection Inference, with Application to the Lasso.” <em>The Annals of Statistics</em> 44 (3): 907–27.
</div>
<div id="ref-li2015flare" class="csl-entry">
Li, Xingguo, Tuo Zhao, Xiaoming Yuan, and Han Liu. 2015. “The Flare Package for High Dimensional Linear Regression and Precision Matrix Estimation in r.” <em>Journal of Machine Learning Research: JMLR</em> 16: 553.
</div>
<div id="ref-lv2009unified" class="csl-entry">
Lv, Jinchi, and Yingying Fan. 2009. “A Unified Approach to Model Selection and Sparse Recovery Using Regularized Least Squares.” <em>The Annals of Statistics</em> 37 (6A): 3498–528.
</div>
<div id="ref-mazumder2011sparsenet" class="csl-entry">
Mazumder, Rahul, Jerome H Friedman, and Trevor Hastie. 2011. “Sparsenet: Coordinate Descent with Nonconvex Penalties.” <em>Journal of the American Statistical Association</em> 106 (495): 1125–38.
</div>
<div id="ref-tibshirani1996regression" class="csl-entry">
Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 58 (1): 267–88.
</div>
<div id="ref-tibshirani2011regression" class="csl-entry">
———. 2011. “Regression Shrinkage and Selection via the Lasso: A Retrospective.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 73 (3): 273–82.
</div>
<div id="ref-wang2018variable" class="csl-entry">
Wang, Yanxin, Qibin Fan, and Li Zhu. 2018. “Variable Selection and Estimation Using a Continuous Approximation to the L0 Penalty.” <em>Annals of the Institute of Statistical Mathematics</em> 70 (1): 191–214.
</div>
<div id="ref-wang2016variable" class="csl-entry">
Wang, Yanxin, and Li Zhu. 2016. “Variable Selection and Parameter Estimation with the Atan Regularization Method.” <em>Journal of Probability and Statistics</em>.
</div>
<div id="ref-williams2020beyond" class="csl-entry">
Williams, Donald R. 2020. “Beyond Lasso: A Survey of Nonconvex Regularization in Gaussian Graphical Models.” <em>PsyArXiv</em>.
</div>
<div id="ref-williams2020back" class="csl-entry">
Williams, Donald R, and Philippe Rast. 2020. “Back to the Basics: Rethinking Partial Correlation Network Methodology.” <em>British Journal of Mathematical and Statistical Psychology</em> 73 (2): 187–212.
</div>
<div id="ref-williams2019nonregularized" class="csl-entry">
Williams, Donald R, Mijke Rhemtulla, Anna C Wysocki, and Philippe Rast.
<ol>
<li>“On Nonregularized Estimation of Psychological Networks.” <em>Multivariate Behavioral Research</em> 54 (5): 719–50.</li>
</ol>
</div>
<div id="ref-zhang2010nearly" class="csl-entry">
Zhang, Cun-Hui. 2010. “Nearly Unbiased Variable Selection Under Minimax Concave Penalty.” <em>The Annals of Statistics</em> 38 (2): 894–942.
</div>
<div id="ref-zhang2018silggm" class="csl-entry">
Zhang, Rong, Zhao Ren, and Wei Chen. 2018. “SILGGM: An Extensive r Package for Efficient Statistical Inference in Large-Scale Gene Networks.” <em>PLoS Computational Biology</em> 14 (8): e1006369.
</div>
<div id="ref-zhao2006model" class="csl-entry">
Zhao, Peng, and Bin Yu. 2006. “On Model Selection Consistency of Lasso.” <em>Journal of Machine Learning Research</em> 7 (Nov): 2541–63.
</div>
<div id="ref-zou2006adaptive" class="csl-entry">
Zou, Hui. 2006. “The Adaptive Lasso and Its Oracle Properties.” <em>Journal of the American Statistical Association</em> 101 (476): 1418–29.
</div>
<div id="ref-zou2008one" class="csl-entry">
Zou, Hui, and Runze Li. 2008. “One-Step Sparse Estimates in Nonconcave Penalized Likelihood Models.” <em>Annals of Statistics</em> 36 (4): 1509.
</div>
</div>
</div>
</div>

  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <div class="links">
<h2>Links</h2>
<ul class="list-unstyled">
<li>Download from CRAN at <br><a href="https://cloud.r-project.org/package=GGMncv">https://​cloud.r-project.org/​package=GGMncv</a>
</li>
<li>Browse source code at <br><a href="https://github.com/donaldRwilliams/GGMncv/">https://​github.com/​donaldRwilliams/​GGMncv/​</a>
</li>
<li>Report a bug at <br><a href="https://github.com/donaldRwilliams/GGMncv/issues">https://​github.com/​donaldRwilliams/​GGMncv/​issues</a>
</li>
</ul>
</div>
<div class="license">
<h2>License</h2>
<ul class="list-unstyled">
<li><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></li>
</ul>
</div>
<div class="developers">
<h2>Developers</h2>
<ul class="list-unstyled">
<li>Donald Williams <br><small class="roles"> Author, maintainer </small>  </li>
</ul>
</div>

  </div>
</div>


      <footer><div class="copyright">
  <p>Developed by Donald Williams.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
